{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee68f778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using legacy setup.py install for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rohit.pandey\\onedrive - azure exponentia ai\\desktop\\devops\\pyspark\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0a58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8252fba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rohit</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sachin</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shivam</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name   age\n",
       "0   rohit   23\n",
       "1  sachin   28\n",
       "2  shivam   25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed273cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfea8717",
   "metadata": {},
   "source": [
    "### if you really want to work with spark we have start the spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070f4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd633c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2256c30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EDL-429.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a667ea3688>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a5a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639dbe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "| name |age|\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca0a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you see above we get column name _c0 or _c1 instead name age because it is taking by default columns name , \n",
    "\n",
    "# to make it name and as collumn we have to do it manually\n",
    "df_pyspark = spark.read.option('header','true').csv('data.csv')    # it will make first row as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d4b2faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "| name |age|\n",
      "+------+---+\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e43e8fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5cccb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(df_pyspark)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baa93662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name ='rohit', age='23'), Row(name ='sachin', age='28')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)  # it will show top 2 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39e1269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets see the schema of df_pyspark\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac123c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d07b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399af70d",
   "metadata": {},
   "source": [
    "## lets cover\n",
    "\n",
    "\n",
    "- pyspark dataframe\n",
    "- reading the dataset\n",
    "- checking the datatype of column(Schema)\n",
    "- check column and indexing\n",
    "- check describe option similar to pandas\n",
    "- adding column dropping column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7f228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb246447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00ebec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4420e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EDL-429.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a667ea3688>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ee9911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.option('header','true').csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eaa38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "| name |age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4be4ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check the schema similear to df.info() in pandas\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6aeb2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default spark will take all values as string thats why be seeing here string \n",
    "# to make datatype as per the value we have to pass one more arguments in csv function called inferSchem = true lets do that \n",
    "\n",
    "spark_df = spark.read.option('header','true').csv('data2.csv', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98d5e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c944ce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n",
      "+------+---+----------+\n",
      "|  name|age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we also don't need of option function to pass header we can pass in csv method also along with inferschema  \n",
    "\n",
    "spark_df = spark.read.option('header','true').csv('data2.csv', header=True, inferSchema='true')\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa6a7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataframe is kind of datastructure to perform various kind of operations \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48d3ea",
   "metadata": {},
   "source": [
    "##### selecting column and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14306477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'Experience']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5ae0708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "| rohit|\n",
      "|sachin|\n",
      "|shivam|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets select the columns \n",
    "\n",
    "spark_df.select('name').show()    # this again the dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7df58b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to select multiple column we have to pass list of colums name\n",
    "spark_df.select(['name','age']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ea1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11579511",
   "metadata": {},
   "source": [
    "#### checking the datatype of columns using dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50c988d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac885b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5757ab",
   "metadata": {},
   "source": [
    "#### check the describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14b8634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, name: string, age: string, Experience: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7f64639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+\n",
      "|summary|  name|               age|       Experience|\n",
      "+-------+------+------------------+-----------------+\n",
      "|  count|     3|                 3|                3|\n",
      "|   mean|  null|25.333333333333332|7.333333333333333|\n",
      "| stddev|  null| 2.516611478423583|3.055050463303893|\n",
      "|    min| rohit|                23|                4|\n",
      "|    max|shivam|                28|               10|\n",
      "+-------+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30530dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a2c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5833afa",
   "metadata": {},
   "source": [
    "#### adding and dropping collumns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee5a5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.withColumn returns the new :class:'dataframe'by adding the column or replacing the existing column that has the same name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe069a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------------+\n",
      "|  name|age|Experience|Experience after 2 year|\n",
      "+------+---+----------+-----------------------+\n",
      "| rohit| 23|        10|                     12|\n",
      "|sachin| 28|         8|                     10|\n",
      "|shivam| 25|         4|                      6|\n",
      "+------+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.withColumn('Experience after 2 year', spark_df['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b8002f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Experience'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df['Experience']   # return only commun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1056c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12317b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping collumns using drop function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85c4072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.drop('Experience after 2 year')\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1c4b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|new Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   rohit| 23|        10|\n",
      "|  sachin| 28|         8|\n",
      "|  shivam| 25|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renaming the column \n",
    "\n",
    "spark_df.withColumnRenamed('name', 'new Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc9929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d03d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39273b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133e122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913880f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acc355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61872f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273624a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1678fd24",
   "metadata": {},
   "source": [
    "## pyspark handling missing values \n",
    "\n",
    "\n",
    "- dropping columns\n",
    "- Dropping rows\n",
    "- Various parameter in Dropping functionalities \n",
    "- handling missing values by mean , mediun, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb318ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ef5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ce7a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "243a231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7333c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|     null|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets read the csv file \n",
    "\n",
    "spark_df = spark.read.csv('data3.csv', header=True, inferSchema=True)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66a61a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in above dataframe clearly we can see the null values lets handle it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e412933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n",
      "| age|Experience| salary|\n",
      "+----+----------+-------+\n",
      "|  23|        10|  25000|\n",
      "|  28|         8|  20000|\n",
      "|  25|         4|  20000|\n",
      "|  32|         3|1000000|\n",
      "|  22|         5| 800000|\n",
      "|  25|         3| 100000|\n",
      "|  56|        10|   null|\n",
      "|null|      null|  40000|\n",
      "|  10|      null|   null|\n",
      "+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# drop the column \n",
    "\n",
    "spark_df.drop('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "246fb590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets drop nan values  Na object has varius methods like replace fill drop\n",
    "\n",
    "spark_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e02dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|     null|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "+---------+----+----------+-------+\n",
      "\n",
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|   mahesh|null|      null|  40000|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop has varius features like na.drop(how='any', thresh=None, subset=None)\n",
    "\n",
    "spark_df.na.drop(how='all').show()# how can have two value any or all , in case of any drop will remove row and all means  if all column contain null then only it will remove \n",
    "\n",
    "spark_df.na.drop(how='any', thresh=2).show() # it will check if in complete row atleast two values is non null then it will not delete else it will delete the rows    \n",
    "\n",
    "spark_df.na.drop(how='any', subset=['age','Experience','salary']).show()  # it will check NA values only in age|Experience| salary \n",
    "spark_df.na.drop(how='any', subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174dde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc7847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e98aafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "|     null| 56|        10|      0|\n",
      "|   mahesh|  0|         0|  40000|\n",
      "|     null| 10|         0|      0|\n",
      "+---------+---+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|  no name|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|  no name|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the missing values  it will take two parameter the values and subset, value from wich na replace , thresold again set of columns \n",
    "\n",
    "# make sure the the type of column should we same if you have enable inferSchema = true while rading \n",
    "\n",
    "spark_df.na.fill(0, ['Experience','age','salary']).show()\n",
    "\n",
    "spark_df.na.fill('no name', ['name']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f0d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa226e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "060d3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the value with mean using imputer function \n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a279f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','age','salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','age','salary'] ]\n",
    ").setStrategy(\"mean\")  # can chenge strategy with mean mediun mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1b2efd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "|     name| age|Experience| salary|Experience_imputed|age_imputed|salary_imputed|\n",
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "|    rohit|  23|        10|  25000|                10|         23|         25000|\n",
      "|   sachin|  28|         8|  20000|                 8|         28|         20000|\n",
      "|   shivam|  25|         4|  20000|                 4|         25|         20000|\n",
      "|sudhandhu|  32|         3|1000000|                 3|         32|       1000000|\n",
      "|    priya|  22|         5| 800000|                 5|         22|        800000|\n",
      "|    shyam|  25|         3| 100000|                 3|         25|        100000|\n",
      "|     null|  56|        10|   null|                10|         56|        286428|\n",
      "|   mahesh|null|      null|  40000|                 6|         27|         40000|\n",
      "|     null|  10|      null|   null|                 6|         10|        286428|\n",
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(spark_df).transform(spark_df).show()  # using this we have replaced the na values with means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a9948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e40ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adabbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73431e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8d117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10894dfa",
   "metadata": {},
   "source": [
    "## pyspark dataframe \n",
    "- Filter Operation \n",
    "- &, |, ==\n",
    "- `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d24064f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a508141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('practice4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "880829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('data4.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3440a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8da212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56f02cb0",
   "metadata": {},
   "source": [
    "### filter opration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a68755da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|Experience|salary|\n",
      "+------+---+----------+------+\n",
      "| rohit| 23|        10| 25000|\n",
      "|sachin| 28|         8| 20000|\n",
      "|shivam| 25|         4| 20000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of the people less than or equal to 25000\n",
    "\n",
    "spark_df.filter(\"salary<=25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7602aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(\"salary==25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e7a9f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "| rohit| 25000|\n",
      "|sachin| 20000|\n",
      "|shivam| 20000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(\"salary<=25000\").select(['name','salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1666e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n",
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n",
      "+------+---+----------+------+\n",
      "|  name|age|Experience|salary|\n",
      "+------+---+----------+------+\n",
      "| rohit| 23|        10| 25000|\n",
      "|sachin| 28|         8| 20000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(spark_df['salary']==25000).show()\n",
    "spark_df.filter((spark_df['salary']==25000) & (spark_df['name']=='rohit')  ).show()\n",
    "spark_df.filter((spark_df['salary']==25000) | (spark_df['name']=='sachin')  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
