{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2015cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using legacy setup.py install for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'done'\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rohit.pandey\\onedrive - azure exponentia ai\\desktop\\devops\\pyspark\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b36577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf83b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rohit</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sachin</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shivam</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name   age\n",
       "0   rohit   23\n",
       "1  sachin   28\n",
       "2  shivam   25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c4721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb07a935",
   "metadata": {},
   "source": [
    "### if you really want to work with spark we have start the spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae5fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4790f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319bfecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EDL-429.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a667ea3688>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7193257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a869804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "| name |age|\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b704668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you see above we get column name _c0 or _c1 instead name age because it is taking by default columns name , \n",
    "\n",
    "# to make it name and as collumn we have to do it manually\n",
    "df_pyspark = spark.read.option('header','true').csv('data.csv')    # it will make first row as header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a722b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "| name |age|\n",
      "+------+---+\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95b2ff6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36e8f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(df_pyspark)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c7a3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name ='rohit', age='23'), Row(name ='sachin', age='28')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)  # it will show top 2 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d927bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets see the schema of df_pyspark\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c1bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3e505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414d0ca5",
   "metadata": {},
   "source": [
    "## lets cover\n",
    "\n",
    "\n",
    "- pyspark dataframe\n",
    "- reading the dataset\n",
    "- checking the datatype of column(Schema)\n",
    "- check column and indexing\n",
    "- check describe option similar to pandas\n",
    "- adding column dropping column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afb9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1248a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49700255",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b1aac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://EDL-429.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a667ea3688>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70003c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.option('header','true').csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41cecbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "| name |age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b320427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check the schema similear to df.info() in pandas\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5ad53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default spark will take all values as string thats why be seeing here string \n",
    "# to make datatype as per the value we have to pass one more arguments in csv function called inferSchem = true lets do that \n",
    "\n",
    "spark_df = spark.read.option('header','true').csv('data2.csv', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51b4daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name : string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1917cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0353ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n",
      "+------+---+----------+\n",
      "|  name|age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we also don't need of option function to pass header we can pass in csv method also along with inferschema  \n",
    "\n",
    "spark_df = spark.read.option('header','true').csv('data2.csv', header=True, inferSchema='true')\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93383a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataframe is kind of datastructure to perform various kind of operations \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a862bcb",
   "metadata": {},
   "source": [
    "##### selecting column and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b33fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'Experience']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a7704ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "| rohit|\n",
      "|sachin|\n",
      "|shivam|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets select the columns \n",
    "\n",
    "spark_df.select('name').show()    # this again the dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4506575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "| rohit| 23|\n",
      "|sachin| 28|\n",
      "|shivam| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to select multiple column we have to pass list of colums name\n",
    "spark_df.select(['name','age']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a286b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe230d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c56cceb4",
   "metadata": {},
   "source": [
    "#### checking the datatype of columns using dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "492c8cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e147e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90424887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54370562",
   "metadata": {},
   "source": [
    "#### check the describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7bd61e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, name: string, age: string, Experience: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "602669a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+\n",
      "|summary|  name|               age|       Experience|\n",
      "+-------+------+------------------+-----------------+\n",
      "|  count|     3|                 3|                3|\n",
      "|   mean|  null|25.333333333333332|7.333333333333333|\n",
      "| stddev|  null| 2.516611478423583|3.055050463303893|\n",
      "|    min| rohit|                23|                4|\n",
      "|    max|shivam|                28|               10|\n",
      "+-------+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cf5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb19ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc26b0b5",
   "metadata": {},
   "source": [
    "#### adding and dropping collumns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93fd987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df.withColumn returns the new :class:'dataframe'by adding the column or replacing the existing column that has the same name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd18cad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------------+\n",
      "|  name|age|Experience|Experience after 2 year|\n",
      "+------+---+----------+-----------------------+\n",
      "| rohit| 23|        10|                     12|\n",
      "|sachin| 28|         8|                     10|\n",
      "|shivam| 25|         4|                      6|\n",
      "+------+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.withColumn('Experience after 2 year', spark_df['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "652e12b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Experience'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df['Experience']   # return only commun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27123515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d0e6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping collumns using drop function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78d941db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|Experience|\n",
      "+------+---+----------+\n",
      "| rohit| 23|        10|\n",
      "|sachin| 28|         8|\n",
      "|shivam| 25|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.drop('Experience after 2 year')\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce1855b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|new Name|age|Experience|\n",
      "+--------+---+----------+\n",
      "|   rohit| 23|        10|\n",
      "|  sachin| 28|         8|\n",
      "|  shivam| 25|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renaming the column \n",
    "\n",
    "spark_df.withColumnRenamed('name', 'new Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d816ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c387f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c090747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88499995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb92a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f598f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4807f0db",
   "metadata": {},
   "source": [
    "## pyspark handling missing values \n",
    "\n",
    "\n",
    "- dropping columns\n",
    "- Dropping rows\n",
    "- Various parameter in Dropping functionalities \n",
    "- handling missing values by mean , mediun, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fbf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4592c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0dd6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4a52970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|     null|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets read the csv file \n",
    "\n",
    "spark_df = spark.read.csv('data3.csv', header=True, inferSchema=True)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f27e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in above dataframe clearly we can see the null values lets handle it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2cb1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n",
      "| age|Experience| salary|\n",
      "+----+----------+-------+\n",
      "|  23|        10|  25000|\n",
      "|  28|         8|  20000|\n",
      "|  25|         4|  20000|\n",
      "|  32|         3|1000000|\n",
      "|  22|         5| 800000|\n",
      "|  25|         3| 100000|\n",
      "|  56|        10|   null|\n",
      "|null|      null|  40000|\n",
      "|  10|      null|   null|\n",
      "+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# drop the column \n",
    "\n",
    "spark_df.drop('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aaf87c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets drop nan values  Na object has varius methods like replace fill drop\n",
    "\n",
    "spark_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "240201c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|     null|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|     null|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "+---------+----+----------+-------+\n",
      "\n",
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|   mahesh|null|      null|  40000|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop has varius features like na.drop(how='any', thresh=None, subset=None)\n",
    "\n",
    "spark_df.na.drop(how='all').show()# how can have two value any or all , in case of any drop will remove row and all means  if all column contain null then only it will remove \n",
    "\n",
    "spark_df.na.drop(how='any', thresh=2).show() # it will check if in complete row atleast two values is non null then it will not delete else it will delete the rows    \n",
    "\n",
    "spark_df.na.drop(how='any', subset=['age','Experience','salary']).show()  # it will check NA values only in age|Experience| salary \n",
    "spark_df.na.drop(how='any', subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e43ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42442e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bbefe3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "|     null| 56|        10|      0|\n",
      "|   mahesh|  0|         0|  40000|\n",
      "|     null| 10|         0|      0|\n",
      "+---------+---+----------+-------+\n",
      "\n",
      "+---------+----+----------+-------+\n",
      "|     name| age|Experience| salary|\n",
      "+---------+----+----------+-------+\n",
      "|    rohit|  23|        10|  25000|\n",
      "|   sachin|  28|         8|  20000|\n",
      "|   shivam|  25|         4|  20000|\n",
      "|sudhandhu|  32|         3|1000000|\n",
      "|    priya|  22|         5| 800000|\n",
      "|    shyam|  25|         3| 100000|\n",
      "|  no name|  56|        10|   null|\n",
      "|   mahesh|null|      null|  40000|\n",
      "|  no name|  10|      null|   null|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling the missing values  it will take two parameter the values and subset, value from wich na replace , thresold again set of columns \n",
    "\n",
    "# make sure the the type of column should we same if you have enable inferSchema = true while rading \n",
    "\n",
    "spark_df.na.fill(0, ['Experience','age','salary']).show()\n",
    "\n",
    "spark_df.na.fill('no name', ['name']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c41dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fb4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dfa0ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the value with mean using imputer function \n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8df65495",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols = ['Experience','age','salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Experience','age','salary'] ]\n",
    ").setStrategy(\"mean\")  # can chenge strategy with mean mediun mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee97e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "|     name| age|Experience| salary|Experience_imputed|age_imputed|salary_imputed|\n",
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "|    rohit|  23|        10|  25000|                10|         23|         25000|\n",
      "|   sachin|  28|         8|  20000|                 8|         28|         20000|\n",
      "|   shivam|  25|         4|  20000|                 4|         25|         20000|\n",
      "|sudhandhu|  32|         3|1000000|                 3|         32|       1000000|\n",
      "|    priya|  22|         5| 800000|                 5|         22|        800000|\n",
      "|    shyam|  25|         3| 100000|                 3|         25|        100000|\n",
      "|     null|  56|        10|   null|                10|         56|        286428|\n",
      "|   mahesh|null|      null|  40000|                 6|         27|         40000|\n",
      "|     null|  10|      null|   null|                 6|         10|        286428|\n",
      "+---------+----+----------+-------+------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(spark_df).transform(spark_df).show()  # using this we have replaced the na values with means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47605463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd03b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6aa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c8ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d60668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cac1e25",
   "metadata": {},
   "source": [
    "## pyspark dataframe \n",
    "- Filter Operation \n",
    "- &, |, ==\n",
    "- `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39b0aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "af9f6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('practice4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eddd604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('data4.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1cf2d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+-------+\n",
      "|     name|age|Experience| salary|\n",
      "+---------+---+----------+-------+\n",
      "|    rohit| 23|        10|  25000|\n",
      "|   sachin| 28|         8|  20000|\n",
      "|   shivam| 25|         4|  20000|\n",
      "|sudhandhu| 32|         3|1000000|\n",
      "|    priya| 22|         5| 800000|\n",
      "|    shyam| 25|         3| 100000|\n",
      "+---------+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b778e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62fff151",
   "metadata": {},
   "source": [
    "### filter opration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4ebefedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  name|age|Experience|salary|\n",
      "+------+---+----------+------+\n",
      "| rohit| 23|        10| 25000|\n",
      "|sachin| 28|         8| 20000|\n",
      "|shivam| 25|         4| 20000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of the people less than or equal to 25000\n",
    "\n",
    "spark_df.filter(\"salary<=25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7258cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(\"salary==25000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "31225697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "| rohit| 25000|\n",
      "|sachin| 20000|\n",
      "|shivam| 20000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(\"salary<=25000\").select(['name','salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f61ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n",
      "+-----+---+----------+------+\n",
      "| name|age|Experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|rohit| 23|        10| 25000|\n",
      "+-----+---+----------+------+\n",
      "\n",
      "+------+---+----------+------+\n",
      "|  name|age|Experience|salary|\n",
      "+------+---+----------+------+\n",
      "| rohit| 23|        10| 25000|\n",
      "|sachin| 28|         8| 20000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.filter(spark_df['salary']==25000).show()\n",
    "spark_df.filter((spark_df['salary']==25000) & (spark_df['name']=='rohit')  ).show()\n",
    "spark_df.filter((spark_df['salary']==25000) | (spark_df['name']=='sachin')  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db63bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
